{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://us-central1-aiplatform.googleapis.com/v1beta1/publishers/google/models/gemini-2.0-flash \"HTTP/1.1 200 OK\"\n",
      "/Users/beckyxu/Documents/GitHub/sgd-insight-engine/python_backend/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:python_backend.document.processor:Cloud Logging not initialized. Using local logging only: name 'cloud_logging' is not defined\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Optional, Any\n",
    "import datetime\n",
    "\n",
    "# from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "# from llama_index.core.retrievers import VectorIndexRetriever, RouterRetriever\n",
    "# from llama_index.core.response_synthesizers import ResponseMode\n",
    "# from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "# from llama_index.core import get_response_synthesizer\n",
    "# from llama_index.core.query_engine import CustomQueryEngine\n",
    "# from llama_index.core.retrievers import BaseRetriever\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('/Users/beckyxu/Documents/GitHub/sgd-insight-engine')\n",
    "\n",
    "from python_backend.config import logger, POLICY_FOLDER, GCP_PROJECT_ID, GCP_LOCATION, DOCUMENTS_BUCKET\n",
    "from python_backend.storage.bigquery import get_fa_from_bigquery\n",
    "from python_backend.storage.gcs import ensure_bucket_exists\n",
    "from python_backend.ai.models import llm, embed_model\n",
    "from python_backend.document.processor import process_document, docling_reader\n",
    "\n",
    "# Initialize global variables\n",
    "_indices_dict = {}\n",
    "_policy_indices_dict = {}\n",
    "_policy_context_cache = None  # Cache for policy context to avoid repeated extraction\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy_docs(\n",
    "    policy_links: List[str] = None, \n",
    "    ) -> Dict:\n",
    "    \"\"\"\n",
    "    Initialize indices for policy documents using the existing ChromaDB collection.\n",
    "    \n",
    "    Args:\n",
    "        policy_links: list of policy document links (not used when loading from existing ChromaDB).\n",
    "\n",
    "    Returns:\n",
    "        List of extracted policy documents for analysis\n",
    "    \"\"\"\n",
    "    # 1. create combinedtext for each document\n",
    "    # file 1 SDG\n",
    "    file_path = policy_links[0]\n",
    "    docs = docling_reader.load_data(file_path)\n",
    "    text_doc1_sdg = ' '.join(doc.text.strip() for doc in docs)\n",
    "    \n",
    "    # file 2 SDG\n",
    "    file_path = policy_links[1]\n",
    "    docs = docling_reader.load_data(file_path)\n",
    "    text_doc2_sdg = ' '.join(doc.text.strip() for doc in docs)\n",
    "    \n",
    "    # file 3 RS\n",
    "    file_path = policy_links[2]\n",
    "    docs = docling_reader.load_data(file_path)\n",
    "    text_doc1_rs = ' '.join(doc.text.strip() for doc in docs)\n",
    "    \n",
    "    # file 4 RS\n",
    "    file_path = policy_links[3]\n",
    "    docs = docling_reader.load_data(file_path)\n",
    "    text_doc2_rs = ' '.join(doc.text.strip() for doc in docs)\n",
    "    \n",
    "    # file 5 RS\n",
    "    file_path = policy_links[4]\n",
    "    docs = docling_reader.load_data(file_path)\n",
    "    text_doc3_rs = ' '.join(doc.text.strip() for doc in docs)\n",
    "    \n",
    "    return [text_doc1_sdg, text_doc2_sdg, text_doc1_rs, text_doc2_rs, text_doc3_rs] \n",
    "\n",
    "def answer_question_from_document_link(document_link: str, policy_doc_list) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Answer a question based on a document link and policy indices.\n",
    "    First extracts information from policy documents, then uses that to analyze the project document.\n",
    "    \n",
    "    Args:\n",
    "        document_link: The link to the project document to analyze.\n",
    "        policy_doc_list: list of extracted document text from\n",
    "                \n",
    "    Returns:\n",
    "        Dict containing the structured analysis and relevant context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: load policy context docs\n",
    "\n",
    "    # Step 2: Download and analyze the project document \n",
    "    logger.info(f\"Processing document link: {document_link}\")\n",
    "    try:\n",
    "        processed_doc = process_document(document_link) \n",
    "        project_doc_text = processed_doc['text_doc_fa'] \n",
    "        project_doc_id = processed_doc['file_id'] \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing project fa doc: {str(e)}\")\n",
    "        return {}\n",
    "    \n",
    "    # Step 3: Extract data from the project fa and the policy docs \n",
    "    try:\n",
    "        system_prompt = \"\"\" Use ReAct:\n",
    "        1. **Reason**: Identify relevant project elements from project document.\n",
    "        2. **Act**: Match to tool or indicators.\n",
    "        3. **Reason**: Validate matches.\n",
    "        4. **Act**: Format the response as JSON\n",
    "        If information is missing or uncertain, include null values.\n",
    "        \"\"\"\n",
    "        from python_backend.ai.models import llm\n",
    "        \n",
    "        # 1. Create a summary text, with measurable indicators \n",
    "        try:\n",
    "            # Create prompt for the LLM\n",
    "            prompt = f\"\"\"{system_prompt}\n",
    "            You are analyzing a project document financial document for Sustainable Development Goals (SDGs) impact and potential application of remote sensing.\n",
    "            The project document contains two main sections in the report: <finance> and <project description>.\n",
    "            Based on the document text below, please answer the following question:\n",
    "\n",
    "            *Questions*:\n",
    "            - Identify quantifiable outcomes (e.g., \"5 health centers built\", \"training for 200 farmers\"). Focus on the <project description> (and <finance> if relevant).\n",
    "            - What are the main objectives of the project? Focus on the <project description>.\n",
    "            - What specific societal, economic, or environmental problems does it address? Focus on <project description>.\n",
    "            - Who are the beneficiaries and who will be impacted? Focus on <project description>.\n",
    "            - What are the anticipated short-term and long-term outcomes? Focus on <project description> and <finance>.\n",
    "\n",
    "            *Here is the document text*:\n",
    "            {project_doc_text}\n",
    "            *Strictly follow this json response format*\n",
    "            {{\n",
    "                \"project_summary\": \"string of brief project SDG impact summary in paragraph\",\n",
    "                \"quantifiable_outcome_list\": [\"outcome_item\": \"string\"],\n",
    "                \"project_summary_list\": [\"analysis_item\": \"string\"]\n",
    "            }}\n",
    "            Create a written summary of the project based on the questions and the document text.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Get response from LLM\n",
    "            response = llm.complete(prompt)\n",
    "            answer_summary = response.text.strip()\n",
    "            \n",
    "            # Store in summary\n",
    "            # print(answer_summary)\n",
    "            logger.info(f\"Generated summary for doc\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Fail to generate summary {str(e)}\")\n",
    "            answer_summary = None\n",
    "            \n",
    "        # 2. Create a sdg and sdg indicators json\n",
    "        try:\n",
    "            # Create prompt for the LLM\n",
    "            prompt = f\"\"\"{system_prompt}\n",
    "            You are analyzing a project document financial document for Sustainable Development Goals (SDGs) and measurable SDG indicators.\n",
    "            You are given two sets of documents: 1. project document, 2. sdg indicators documents.\n",
    "            The project document contains two main sections in the report: <finance> and <project description>.\n",
    "            Focusing on <project description> and the sdg indicators documents, analyze:\n",
    "            - What SDG goals does this project contribute to?\n",
    "            - What specific SDG indicators are measurable in this project?\"\n",
    "            - Output a nested json with keys: \"sdg_goals\" and \"sdg_indicators\". Each key can have a list of objects. Follow the format below.\n",
    "\n",
    "            *Here is the project document*:\n",
    "            {project_doc_text}\n",
    "\n",
    "            *Here is the sdg indicators document*:\n",
    "            {policy_doc_list[0]}\n",
    "            {policy_doc_list[1]}\n",
    "\n",
    "            *Strictly follow this json response format*\n",
    "            {{\n",
    "                \"sdg_goals\": [\n",
    "                    {{\n",
    "                        \"sdg_goal\": \"string\",  # e.g., \"6\"\n",
    "                        \"name\": \"string\",     # e.g., \"Clean Water and Sanitation\"\n",
    "                        \"relevance\": \"string\" # e.g., \"Provides safe water\"\n",
    "                    }}\n",
    "                ],\n",
    "                \"sdg_indicators\": [\n",
    "                    {{\n",
    "                        \"sdg_indicator\": \"string\",\n",
    "                        \"description\" : \"string\",\n",
    "                        \"measurability\" : \"string\"\n",
    "                    }}\n",
    "                ]\n",
    "            }}\n",
    "\n",
    "            Only output the json string.\n",
    "            \"\"\"\n",
    "            # Get response from LLM\n",
    "            response = llm.complete(prompt)\n",
    "            answer_sdg = response.text.strip()\n",
    "            \n",
    "            # Store in summary\n",
    "            # print(answer_sdg)\n",
    "            logger.info(f\"Generated SDG for doc\")  \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating SDG doc: {str(e)}\")\n",
    "            answer_sdg = None\n",
    "            \n",
    "        # 3. Create a imat and remote sensing capacity json\n",
    "        try:\n",
    "            # Create prompt for the LLM\n",
    "            prompt = f\"\"\"{system_prompt}\n",
    "            You are analyzing a project document financial document for potential application of remote sensing.\n",
    "            You are given two sets of documents: 1. project document, 2. remote sensing tools documents.\n",
    "            The project document contains two main sections in the report: <finance> and <project description>.\n",
    "            Focusing on <project description> from the  project document, and the remote sensing tool documents, analyze:\n",
    "            - Which remote sensing or other IMAT tools applicable for this project?\n",
    "            - How is this remote sensing or IMAT tool applicable for this project?\n",
    "            - Output a nested json with keys: \"remote_sensing_tools\". Each key can have a list of objects. Follow the format below.\n",
    "\n",
    "            *Here is the project document text*:\n",
    "            {project_doc_text}\n",
    "\n",
    "            *Here are the remote sensing tools documents*:\n",
    "            {policy_doc_list[2]}\n",
    "            {policy_doc_list[3]}\n",
    "            {policy_doc_list[4]}\n",
    "\n",
    "            *Strictly follow this nested json response format*\n",
    "                {{\n",
    "                    \"remote_sensing_tools\":[\n",
    "                        {{\n",
    "                            \"technology\": \"string\",   # e.g., \"Remote Sensing Tool A\"\n",
    "                            \"application\": \"string\"   # e.g., \"Monitor water sources\"\n",
    "                        }}\n",
    "                    ]\n",
    "                }}\n",
    "            Only output the json string.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Get response from LLM\n",
    "            response = llm.complete(prompt)\n",
    "            answer_rs = response.text.strip()\n",
    "            \n",
    "            # Store in summary\n",
    "            # print(answer_rs)\n",
    "            logger.info(f\"Generated remote sensing summary for doc\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error remote sensing summary doc: {str(e)}\")\n",
    "            answer_rs = None\n",
    "\n",
    "        answer_summary_dict = safe_json_parse(answer_summary) if answer_summary else {}\n",
    "        answer_sdg_dict = safe_json_parse(answer_sdg) if answer_sdg else {}\n",
    "        answer_rs_dict = safe_json_parse(answer_rs) if answer_rs else {}\n",
    "        file_id_dict = {\"file_id\": project_doc_id}\n",
    "        combined_result = {**file_id_dict, **answer_summary_dict, **answer_sdg_dict, **answer_rs_dict}\n",
    "        \n",
    "        result = json.dumps(combined_result)\n",
    "        # result = json.loads(result)\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing document: {str(e)}\")\n",
    "        return {\"answer\": f\"Error processing document: {str(e)}\", \"source_links\": [document_link]}\n",
    "    \n",
    "import json\n",
    "def safe_json_parse(response_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts JSON from a code block in an LLM response and parses it.\n",
    "    \n",
    "    Args:\n",
    "        response_text: The text returned by the LLM (possibly with ```json ... ```)\n",
    "\n",
    "    Returns:\n",
    "        Parsed Python dictionary, or empty dict if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Match JSON block wrapped in triple backticks\n",
    "        match = re.search(r\"```(?:json)?\\s*({.*?})\\s*```\", response_text, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "            return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract/parse JSON: {e}\")\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = policy_links[0]\n",
    "# docs = docling_reader.load_data(file_path)\n",
    "# text_doc1_sdg = ' '.join(doc.text.strip() for doc in docs)\n",
    "# print(text_doc1_sdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE ONLY RUN THIS ONCE\n",
    "\n",
    "policy_links = [\n",
    "    '/Users/beckyxu/Documents/GitHub/sgd-insight-engine/python_backend/policy_docs/Global-Indicator-Framework-after-2024-refinement-English.xlsx',\n",
    "    '/Users/beckyxu/Documents/GitHub/sgd-insight-engine/python_backend/policy_docs/Guidance for aligning UNOPS engagements to SDGs.docx',\n",
    "    '/Users/beckyxu/Documents/GitHub/sgd-insight-engine/python_backend/policy_docs/IMATâ€™s Remote Sensing Capacities and Tools.docx',\n",
    "    '/Users/beckyxu/Documents/GitHub/sgd-insight-engine/python_backend/policy_docs/Rescuing the SDGs with Geospatial Information (1).pdf',\n",
    "    '/Users/beckyxu/Documents/GitHub/sgd-insight-engine/python_backend/policy_docs/UNOPS Geographic Information Systems (GIS).pptx'\n",
    "]\n",
    "policy_doc_list = create_policy_docs(policy_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_link = \"https://drive.google.com/file/d/1BnxsDqskNB2Q4KLcZ7mAK-tZ6BGXaiEY\"\n",
    "if not is_document_already_processed(document_link):\n",
    "    result = answer_question_from_document_link(document_link, policy_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(result))\n",
    "result1 = json.dumps(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue here to figure out how to upload to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output filename\n",
    "output_filename = '/Users/beckyxu/Documents/GitHub/sgd-insight-engine/python_backend/policy_docs/test_project_data.json'\n",
    "\n",
    "# try:\n",
    "#     # 1. Load the string into a Python dictionary to validate it's proper JSON\n",
    "#     #    and to allow for pretty-printing.\n",
    "#     data = json.loads(result1)\n",
    "\n",
    "#     # 2. Write the Python dictionary to a file as formatted JSON\n",
    "#     #    'w' mode overwrites the file if it exists.\n",
    "#     #    encoding='utf-8' is standard for JSON.\n",
    "#     #    indent=4 makes the output file human-readable.\n",
    "#     with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "#     print(f\"Data successfully validated, formatted, and saved to: {os.path.abspath(output_filename)}\")\n",
    "\n",
    "# except json.JSONDecodeError as e:\n",
    "#     print(f\"Error: The provided string is not valid JSON. {e}\")\n",
    "#     # Optional: Save the original (potentially invalid) string anyway\n",
    "#     # with open(\"project_data_raw_error.txt\", 'w', encoding='utf-8') as f_err:\n",
    "#     #     f_err.write(json_data_string)\n",
    "#     # print(\"Original raw string saved to project_data_raw_error.txt\")\n",
    "\n",
    "# except IOError as e:\n",
    "#     print(f\"Error writing file {output_filename}: {e}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it back\n",
    "\n",
    "output_filename = '/Users/beckyxu/Documents/GitHub/sgd-insight-engine/python_backend/policy_docs/test_project_data.json'\n",
    "\n",
    "# NOTE: The original file is formatted with a string outside with json.dump() ... '{\"file_id\":\n",
    "\n",
    "filename = output_filename\n",
    "\n",
    "try:\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Now 'data' contains the JSON data as a Python dictionary\n",
    "    print(\"JSON data loaded successfully:\")\n",
    "    # Print a few keys/values for verification (don't print the whole thing if it's huge)\n",
    "    print(f\"All data: {json.dumps(data, indent=4)}\") # Print another field\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File '{filename}' not found.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error: Invalid JSON in file '{filename}': {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['file_id'] = 'https://drive.google.com/file/d/1BnxsDqskNB2Q4KLcZ7mAK-tZ6BGXaiEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_backend.storage.bigquery import create_bigquery_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_backend.storage.bigquery import get_bigquery_client, ensure_processed_docs_table_exists, is_document_already_processed, mark_document_as_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import main\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from config import logger, GCP_PROJECT_ID, BQ_FA_DATASET, BQ_FA_TABLE, GCP_REPORTS_ID \n",
    "\n",
    "def upload_to_bigquery(row: dict, project_id = GCP_PROJECT_ID, dataset_id = BQ_FA_DATASET, table_id = \"analysis_results\") -> None:\n",
    "    \"\"\"\n",
    "    Upload a single row to a BigQuery table with Repeated Fields.\n",
    "    Args:\n",
    "        row: Dict[str, Any], processed row from json document analysis results to upload,\n",
    "        project_id: str, GCP project id, \n",
    "        dataset_id: str, GCS dataset id, \n",
    "        table_id: str, GCS table id\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    \n",
    "    bigquery_client = get_bigquery_client()\n",
    "    if not bigquery_client or not ensure_processed_docs_table_exists():\n",
    "        logger.warning(\"BigQuery client or processed documents table not initialized\")\n",
    "        return False\n",
    "            \n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=schema,\n",
    "        write_disposition=\"WRITE_APPEND\"\n",
    "    )\n",
    "    try:\n",
    "        job = bigquery_client.load_table_from_json([row], table_ref, job_config=job_config)\n",
    "        job.result()  # Wait for the job to complete.  Raises exception if it fails.\n",
    "        logger.info(f\"Uploaded row for {row['document_link']} to {table_ref}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error uploading to BigQuery: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"file_id\", \"STRING\", mode=\"REQUIRED\", description=\"Unique identifier for the file\"),\n",
    "    bigquery.SchemaField(\"project_summary\", \"STRING\", mode=\"NULLABLE\", description=\"A summary of the project\"),\n",
    "    bigquery.SchemaField(\n",
    "        \"quantifiable_outcome_list\",\n",
    "        \"RECORD\",\n",
    "        mode=\"REPEATED\",\n",
    "        description=\"List of quantifiable outcomes\",\n",
    "        fields=[\n",
    "            bigquery.SchemaField(\"outcome_item\", \"STRING\", mode=\"NULLABLE\", description=\"A specific quantifiable outcome item\")\n",
    "        ],\n",
    "    ),\n",
    "    bigquery.SchemaField(\n",
    "        \"project_summary_list\",\n",
    "        \"RECORD\",\n",
    "        mode=\"REPEATED\",\n",
    "        description=\"List of project summary analysis items\",\n",
    "        fields=[\n",
    "            bigquery.SchemaField(\"analysis_item\", \"STRING\", mode=\"NULLABLE\", description=\"An analysis item related to the project summary\")\n",
    "        ],\n",
    "    ),\n",
    "    bigquery.SchemaField(\n",
    "        \"sdg_goals\",\n",
    "        \"RECORD\",\n",
    "        mode=\"REPEATED\",\n",
    "        description=\"List of Sustainable Development Goals associated with the project\",\n",
    "        fields=[\n",
    "            bigquery.SchemaField(\"sdg_goal\", \"STRING\", mode=\"NULLABLE\", description=\"The SDG goal number (e.g., 16)\"),\n",
    "            bigquery.SchemaField(\"name\", \"STRING\", mode=\"NULLABLE\", description=\"The name of the SDG goal (e.g., Peace, Justice and Strong Institutions)\"),\n",
    "            bigquery.SchemaField(\"relevance\", \"STRING\", mode=\"NULLABLE\", description=\"Explanation of how the project relates to the SDG goal\"),\n",
    "        ],\n",
    "    ),\n",
    "    bigquery.SchemaField(\n",
    "        \"sdg_indicators\",\n",
    "        \"RECORD\",\n",
    "        mode=\"REPEATED\",\n",
    "        description=\"List of Sustainable Development Goal indicators associated with the project\",\n",
    "        fields=[\n",
    "            bigquery.SchemaField(\"sdg_indicator\", \"STRING\", mode=\"NULLABLE\", description=\"The SDG indicator number (e.g., 16.4.1)\"),\n",
    "            bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\", description=\"Description of the SDG indicator\"),\n",
    "            bigquery.SchemaField(\"measurability\", \"STRING\", mode=\"NULLABLE\", description=\"Explanation of how the indicator can be measured in relation to the project\"),\n",
    "        ],\n",
    "    ),\n",
    "    bigquery.SchemaField(\n",
    "        \"remote_sensing_tools\",\n",
    "        \"RECORD\",\n",
    "        mode=\"REPEATED\",\n",
    "        description=\"List of remote sensing tools used in the project\",\n",
    "        fields=[\n",
    "            bigquery.SchemaField(\"technology\", \"STRING\", mode=\"NULLABLE\", description=\"The name of the remote sensing technology or tool\"),\n",
    "            bigquery.SchemaField(\"application\", \"STRING\", mode=\"NULLABLE\", description=\"How the tool is applied in the project\"),\n",
    "        ],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_bigquery(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_backend.storage.bigquery import get_bigquery_client, is_document_already_processed, mark_document_as_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link='https://drive.google.com/file/d/1BnxsDqskNB2Q4KLcZ7mAK-tZ6BGXaiEY'\n",
    "is_document_already_processed(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_document_as_processed(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_backend.auth.credentials import credentials_manager\n",
    "from python_backend.config import logger, GCP_PROJECT_ID, GCP_LOCATION, BQ_FA_DATASET, BQ_FA_TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.table.RowIterator object at 0x31758f7d0>\n"
     ]
    }
   ],
   "source": [
    "# Use the GCP_LOCATION from config to specify the location explicitly\n",
    "bigquery_client_fa = get_bigquery_client()\n",
    "if not bigquery_client_fa:\n",
    "    print(\"error initializing client\")\n",
    "\n",
    "# TODO: CHANGE THIS TO THE ACTUAL COLNAME\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "  Legal_Agreement,\n",
    "  t0.File_URL,\n",
    "  Region,\n",
    "  t1.Donor\n",
    "FROM\n",
    "  `unops-eil-sdg-measurement-prod`.NS_Project_Legal_Agreement.TBL_Project_Legal_Agreement\n",
    "  CROSS JOIN\n",
    "  UNNEST(Legal_Agreement_Files) AS t0\n",
    "  CROSS JOIN\n",
    "  UNNEST(Donors) AS t1\n",
    "WHERE\n",
    "  t0.File_URL IS NOT NULL\n",
    "LIMIT 2;\n",
    "\"\"\"\n",
    "\n",
    "query_job = bigquery_client_fa.query(query) \n",
    "results = query_job.result()\n",
    "print(results)\n",
    "# links = [row .file_url for row in results]\n",
    "\n",
    "# links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = list(results)\n",
    "links = [row.Legal_Agreement for row in result_list]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Please install the 'db-dtypes' package to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/sgd-insight-engine/python_backend/venv/lib/python3.12/site-packages/google/cloud/bigquery/_pandas_helpers.py:59\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb_dtypes\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     61\u001b[39m     date_dtype_name = db_dtypes.DateDtype.name\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'db_dtypes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mresults\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/sgd-insight-engine/python_backend/venv/lib/python3.12/site-packages/google/cloud/bigquery/table.py:2529\u001b[39m, in \u001b[36mRowIterator.to_dataframe\u001b[39m\u001b[34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[39m\n\u001b[32m   2294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_dataframe\u001b[39m(\n\u001b[32m   2295\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2296\u001b[39m     bqstorage_client: Optional[\u001b[33m\"\u001b[39m\u001b[33mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2315\u001b[39m     ] = DefaultPandasDTypes.RANGE_TIMESTAMP_DTYPE,\n\u001b[32m   2316\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mpandas.DataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2317\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a pandas DataFrame by loading all pages of a query.\u001b[39;00m\n\u001b[32m   2318\u001b[39m \n\u001b[32m   2319\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2527\u001b[39m \n\u001b[32m   2528\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2529\u001b[39m     \u001b[43m_pandas_helpers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverify_pandas_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2531\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m geography_as_object \u001b[38;5;129;01mand\u001b[39;00m shapely \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2532\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_SHAPELY_ERROR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/sgd-insight-engine/python_backend/venv/lib/python3.12/site-packages/google/cloud/bigquery/_pandas_helpers.py:1144\u001b[39m, in \u001b[36mverify_pandas_imports\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_PANDAS_ERROR) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas_import_exception\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m db_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_NO_DB_TYPES_ERROR) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdb_dtypes_import_exception\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Please install the 'db-dtypes' package to use this function."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = results.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Iterator has already started', <google.cloud.bigquery.table.RowIterator object at 0x31758d220>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m links = \u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLegal_Agreement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/sgd-insight-engine/python_backend/venv/lib/python3.12/site-packages/google/api_core/page_iterator.py:223\u001b[39m, in \u001b[36mIterator.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Iterator for each item returned.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m    216\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m \u001b[33;03m    ValueError: If the iterator has already been started.\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._started:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIterator has already started\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    224\u001b[39m \u001b[38;5;28mself\u001b[39m._started = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._items_iter()\n",
      "\u001b[31mValueError\u001b[39m: ('Iterator has already started', <google.cloud.bigquery.table.RowIterator object at 0x31758d220>)"
     ]
    }
   ],
   "source": [
    "links = [row.Legal_Agreement for row in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_fa_from_bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_location(dataset_id):\n",
    "    \"\"\"Retrieves the location of a BigQuery dataset.\"\"\"\n",
    "\n",
    "    bigquery_client = get_bigquery_client()\n",
    "    dataset_ref = bigquery_client.dataset(dataset_id)\n",
    "    dataset = bigquery_client.get_dataset(dataset_ref)  # Make an API request.\n",
    "\n",
    "    print(f\"Dataset {dataset_id} is located in {dataset.location}\")\n",
    "    return dataset.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "_bigquery_client = None\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def get_bigquery_client():\n",
    "    \"\"\"Get authenticated BigQuery client.\"\"\"\n",
    "    # a global variable is accessible throughout the entire module (file)\n",
    "    global _bigquery_client\n",
    "    if _bigquery_client is None:\n",
    "        try:\n",
    "            credentials = credentials_manager.get_credentials('bigquery')\n",
    "            _bigquery_client = bigquery.Client(project=GCP_PROJECT_ID, credentials=credentials)\n",
    "            logger.info(\"BigQuery client initialized\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing BigQuery client: {str(e)}\")\n",
    "            return None\n",
    "    return _bigquery_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset NS_Project_Legal_Agreement is located in EU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'EU'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_location(BQ_FA_DATASET)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDG Insight Engine",
   "language": "python",
   "name": "sdg-insight-engine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
